Microsoft Windows [VersiÃ³n 10.0.19044.1706]
(c) Microsoft Corporation. Todos los derechos reservados.

C:\Users\unai.iparraguirre>ipconfig>C:\CCM\CMD\iepi.txt

C:\Users\unai.iparraguirre>pyspark
Python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license" for more information.
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/C:/spark-3.2.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
22/05/30 13:42:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
22/05/30 13:42:06 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.2.1
      /_/

Using Python version 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022 23:13:41)
Spark context Web UI available at http://L2205005.mshome.net:4041
Spark context available as 'sc' (master = local[*], app id = local-1653910926703).
SparkSession available as 'spark'.
>>> 22/05/30 13:42:23 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped

>>> spark = SparkSession.builder.appName("PythonMnMCount").getOrCreate();
>>> spark = SparkSession.builder.appName("PythonMnMCount").getOrCreate();
>>> mnm_file = "C:\Users\unai.iparraguirre\Documents\BIG DATA\LearningSparkV2-master\chapter2\py\src\data\mnm_dataset.cvs"
  File "<stdin>", line 1
    mnm_file = "C:\Users\unai.iparraguirre\Documents\BIG DATA\LearningSparkV2-master\chapter2\py\src\data\mnm_dataset.cvs"
                                                                                                                          ^
SyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \UXXXXXXXX escape
SyntaxError: invalid syntax
>>> mnm_df = spark.read.format("csv").option("header","true").option("inferSchema","true").load("C:\Users\unai.iparraguirre\Documents\BIG DATA\LearningSparkV2-master\chapter2\py\src\data\mnm_dataset.csv");
  File "<stdin>", line 1
    mnm_df = spark.read.format("csv").option("header","true").option("inferSchema","true").load("C:\Users\unai.iparraguirre\Documents\BIG DATA\LearningSparkV2-master\chapter2\py\src\data\mnm_dataset.csv");
                                                                                                                                                                                                           ^
SyntaxError: (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \UXXXXXXXX escape


#######OJO HAY QUE TRATAR LA RUTA, BIEN PONIENDO UNA r AL FINAL O AADIENDO UNA DOBLE \\ O DIRECTAMENTE ESCRIBIENDOLO CON / ################

>>> mnm_df = spark.read.format("csv").option("header","true").option("inferSchema","true").load(r"C:\Users\unai.iparraguirre\Documents\BIG DATA\LearningSparkV2-master\chapter2\py\src\data\mnm_dataset.csv");
>>> count_mnm_df = mnm_df.select("State","Color","Count").groupBy("State","Color").agg(count("Count").alias("Total")).orderBy("Total",ascending=False);
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'count' is not defined. Did you mean: 'round'?


#############IMPORTAMOS EL COUNT!!#########################

>>> from pyspark.sql.functions import count
>>> count_mnm_df = mnm_df.select("State","Color","Count").groupBy("State","Color").agg(count("Count").alias("Total")).orderBy("Total",ascending=False);
>>> count_mnm_df.show(n=60, truncate=False)
+-----+------+-----+
|State|Color |Total|
+-----+------+-----+
|CA   |Yellow|1807 |
|WA   |Green |1779 |
|OR   |Orange|1743 |
|TX   |Green |1737 |
|TX   |Red   |1725 |
|CA   |Green |1723 |
|CO   |Yellow|1721 |
|CA   |Brown |1718 |
|CO   |Green |1713 |
|NV   |Orange|1712 |
|TX   |Yellow|1703 |
|NV   |Green |1698 |
|AZ   |Brown |1698 |
|WY   |Green |1695 |
|CO   |Blue  |1695 |
|NM   |Red   |1690 |
|AZ   |Orange|1689 |
|NM   |Yellow|1688 |
|NM   |Brown |1687 |
|UT   |Orange|1684 |
|NM   |Green |1682 |
|UT   |Red   |1680 |
|AZ   |Green |1676 |
|NV   |Yellow|1675 |
|NV   |Blue  |1673 |
|WA   |Red   |1671 |
|WY   |Red   |1670 |
|WA   |Brown |1669 |
|NM   |Orange|1665 |
|WY   |Blue  |1664 |
|WA   |Yellow|1663 |
|WA   |Orange|1658 |
|CA   |Orange|1657 |
|NV   |Brown |1657 |
|CA   |Red   |1656 |
|CO   |Brown |1656 |
|UT   |Blue  |1655 |
|AZ   |Yellow|1654 |
|TX   |Orange|1652 |
|AZ   |Red   |1648 |
|OR   |Blue  |1646 |
|UT   |Yellow|1645 |
|OR   |Red   |1645 |
|CO   |Orange|1642 |
|TX   |Brown |1641 |
|NM   |Blue  |1638 |
|AZ   |Blue  |1636 |
|OR   |Green |1634 |
|UT   |Brown |1631 |
|WY   |Yellow|1626 |
|WA   |Blue  |1625 |
|CO   |Red   |1624 |
|OR   |Brown |1621 |
|TX   |Blue  |1614 |
|OR   |Yellow|1614 |
|NV   |Red   |1610 |
|CA   |Blue  |1603 |
|WY   |Orange|1595 |
|UT   |Green |1591 |
|WY   |Brown |1532 |
+-----+------+-----+

>>> count_mnm_df.count()
60
>>> ca_count_mnm_df = mnm_df.select("State","Color","Count").where(mnm_df.State =="CA").groupBy("State","Color").agg(count("Count").alias("Total")).orderBy("Total",ascending=False)
>>> ca_count_mnm_df.show(n=10,truncate=False)
+-----+------+-----+
|State|Color |Total|
+-----+------+-----+
|CA   |Yellow|1807 |
|CA   |Green |1723 |
|CA   |Brown |1718 |
|CA   |Orange|1657 |
|CA   |Red   |1656 |
|CA   |Blue  |1603 |
+-----+------+-----+

>>>